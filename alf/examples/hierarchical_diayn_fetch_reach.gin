import alf.algorithms.sac_algorithm
import alf.trainers.policy_trainer
import alf.algorithms.diayn_algorithm

import alf.algorithms.learned_goal_generator
import alf.algorithms.hierarchical_agent
import alf.networks.critic_networks
import alf.environments.suite_robotics



# skill related
num_of_skills=20
skill_feature_size=%num_of_skills # one-hot representation

# environment config
NUM_PARALLEL_ENVIRONMENTS=20
create_environment.env_load_fn=@suite_robotics.load
create_environment.num_parallel_environments=%NUM_PARALLEL_ENVIRONMENTS
create_environment.env_name='FetchReach-v1'


# algorithm config
observation_spec=@get_observation_spec()
action_spec=@get_action_spec()
goal/alf.TensorSpec.shape=(%skill_feature_size,)
K=10
create_time_step_spec.num_of_steps=%K

feature_size=200


lowlevel_rl_input_specs=[%observation_spec, @goal/alf.TensorSpec()]

actor/ActorDistributionNetwork.input_tensor_spec=%lowlevel_rl_input_specs
actor/ActorDistributionNetwork.preprocessing_combiner=@actor/NestConcat()
actor/ActorDistributionNetwork.action_spec=%action_spec
actor/ActorDistributionNetwork.fc_layer_params=(%feature_size, %feature_size)
actor/ActorDistributionNetwork.continuous_projection_net_ctor=@NormalProjectionNetwork

NormalProjectionNetwork.squash_mean=True
NormalProjectionNetwork.state_dependent_std=True
NormalProjectionNetwork.scale_distribution=False
NormalProjectionNetwork.std_transform=@clipped_exp


critic/CriticNetwork.input_tensor_spec=(%lowlevel_rl_input_specs, %action_spec)
critic/CriticNetwork.observation_preprocessing_combiner=@critic/NestConcat()
critic/CriticNetwork.joint_fc_layer_params=((%feature_size, %feature_size))

SacAlgorithm.actor_network=@actor/ActorDistributionNetwork()
SacAlgorithm.critic_network=@critic/CriticNetwork()
SacAlgorithm.target_update_tau=0.005


diayn/encoding_net_fc_layer_params=(%feature_size,)
diayn/EncodingNetwork.input_tensor_spec=%observation_spec
diayn/EncodingNetwork.fc_layer_params=%diayn/encoding_net_fc_layer_params
diayn/create_discrete_skill_spec.num_of_skills=%num_of_skills

DIAYNAlgorithm.skill_spec=@diayn/create_discrete_skill_spec()
#DIAYNAlgorithm.feature_spec=@diayn/alf.TensorSpec()
DIAYNAlgorithm.encoding_net=@diayn/EncodingNetwork()
DIAYNAlgorithm.hidden_size=(%feature_size,)

goal/create_goal_spec.num_of_skills=%num_of_skills
goal_actor/ActorDistributionNetwork.input_tensor_spec=%observation_spec
goal_actor/ActorDistributionNetwork.action_spec=@goal/create_goal_spec()
goal_actor/ActorDistributionNetwork.fc_layer_params=((%feature_size, %feature_size))
goal_actor/Adam.lr=3e-4


#goal_critic/NestConcatenate.axis=-1
goal_critic/QNetwork.input_tensor_spec=%observation_spec
goal_critic/QNetwork.action_spec=@goal/create_goal_spec()
goal_critic/QNetwork.fc_layer_params=(%feature_size, %feature_size)
goal_critic/Adam.lr=3e-4

goal_alpha/Adam.lr=3e-4
goal/SacAlgorithm.actor_network=@goal_actor/ActorDistributionNetwork()
goal/SacAlgorithm.actor_optimizer=@goal_actor/Adam()
goal/SacAlgorithm.critic_optimizer=@goal_critic/Adam()
goal/SacAlgorithm.alpha_optimizer=@goal_alpha/Adam()
#goal/SacAlgorithm.critic_network=@goal_critic/alf.networks.critic_network.CriticNetwork()
goal/SacAlgorithm.critic_network=@goal_critic/QNetwork()
goal/SacAlgorithm.debug_summaries=True
goal/SacAlgorithm.observation_spec=%observation_spec
goal/SacAlgorithm.action_spec=@goal/create_goal_spec()
LearnedCategoricalGoalGenerator.observation_spec=%observation_spec
LearnedCategoricalGoalGenerator.num_of_goals=%num_of_skills
LearnedCategoricalGoalGenerator.num_steps_before_policy_switch=%K
LearnedCategoricalGoalGenerator.mini_batch_length=2
LearnedCategoricalGoalGenerator.mini_batch_size=256
LearnedCategoricalGoalGenerator.rl_algorithm_cls=@goal/SacAlgorithm

OneStepTDLoss.td_error_loss_fn=@losses.element_wise_squared_loss


# agent config with the goal generator
HierarchicalAgent/Adam.lr=5e-4
#HierarchicalAgent.intrinsic_reward_coef=1
#HierarchicalAgent.extrinsic_reward_coef=0
HierarchicalAgent.action_spec=%action_spec
HierarchicalAgent.observation_spec=%observation_spec
HierarchicalAgent.rl_algorithm_cls=@SacAlgorithm
HierarchicalAgent.intrinsic_reward_module=@DIAYNAlgorithm()
HierarchicalAgent.goal_generator=@LearnedCategoricalGoalGenerator()
HierarchicalAgent.optimizer=@HierarchicalAgent/Adam()


# training config
TrainerConfig.initial_collect_steps=1000
TrainerConfig.mini_batch_length=2
TrainerConfig.unroll_length=1
TrainerConfig.mini_batch_size=128
TrainerConfig.num_updates_per_train_step=1
TrainerConfig.whole_replay_buffer_training=False
TrainerConfig.clear_replay_buffer=False
TrainerConfig.algorithm_ctor=@HierarchicalAgent
TrainerConfig.num_iterations=100000
TrainerConfig.evaluate=0
TrainerConfig.eval_interval=500
TrainerConfig.debug_summaries=True
TrainerConfig.summarize_grads_and_vars=1
TrainerConfig.summary_interval=500
TrainerConfig.use_rollout_state=True

#goal/SacAlgorithm.config=@TrainerConfig

ReplayBuffer.max_length=1000000


